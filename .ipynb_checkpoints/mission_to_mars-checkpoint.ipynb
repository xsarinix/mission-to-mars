{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object for NASA website\n",
    "nasa_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "nasa_html = requests.get(nasa_url).text\n",
    "nasa_soup = bs(nasa_html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NASA's InSight Places First Instrument on Mars\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get first title\n",
    "title_results = nasa_soup.find_all('div', class_=\"content_title\")\n",
    "title_list = []\n",
    "for result in title_results:\n",
    "    try:\n",
    "        title = result.find('a').text.strip()\n",
    "        if title:\n",
    "            # print(title)\n",
    "            title_list.append(title)\n",
    "    except Exception as e:\n",
    "        print(f'Fail: {e}')\n",
    "news_title = title_list[0]\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In deploying its first instrument onto the surface of Mars, the lander completes a major mission milestone.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get first paragraph\n",
    "p_results = nasa_soup.find_all('div', class_=\"rollover_description_inner\")\n",
    "p_list = []\n",
    "for p in p_results:\n",
    "    try:\n",
    "        par = p.text.strip()\n",
    "        if par:\n",
    "            p_list.append(par)\n",
    "    except Exception as e:\n",
    "        print(f'Fail: {e}')\n",
    "news_p = p_list[0]\n",
    "news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splinter broswer instance\n",
    "executable_path = {'executable_path':'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://photojournal.jpl.nasa.gov/jpeg/PIA19913.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape featured image from NASA\n",
    "nasa_images_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(nasa_images_url)\n",
    "# Click button for full image\n",
    "browser.find_by_css('.button').first.click()\n",
    "# Pause to let load\n",
    "time.sleep(3)\n",
    "# Click more info button\n",
    "browser.find_by_css('.button').last.click()\n",
    "# Get image name from \"Image Details\" and use to navigate to image URL\n",
    "partial_link = browser.find_by_css('.download_tiff').last.value.split(\" \")[2]\n",
    "browser.click_link_by_partial_href(partial_link)\n",
    "featured_image_url = browser.url\n",
    "browser.quit()\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object for Twitter site\n",
    "twitter_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "twitter_html = requests.get(twitter_url).text\n",
    "twitter_soup = bs(twitter_html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sol 2305 (2019-01-30), high -4C/24F, low -73C/-99F, pressure at 8.14 hPa, daylight 06:47-18:54pic.twitter.com/OTkUTDyRpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tweets\n",
    "tweets = twitter_soup.find_all('div', class_ = \"content\")\n",
    "weather_only_tweets = []\n",
    "\n",
    "# Loop through tweets to find weather report tweets\n",
    "for tweet in tweets:\n",
    "    # Eliminate retweets\n",
    "    username = tweet.find('span', class_ = \"username u-dir u-textTruncate\")\n",
    "    if username.text == \"@MarsWxReport\":\n",
    "        tweet_content = tweet.find('p', class_ = \"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text.strip()\n",
    "        # Eliminate non-weather tweets\n",
    "        report_test = tweet_content.split(\" \")\n",
    "        if report_test[0] == \"Sol\":\n",
    "            weather_only_tweets.append(tweet_content)\n",
    "mars_weather = weather_only_tweets[0]\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">  <thead>    <tr>      <th>0</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Equatorial Diameter:</th>      <td>6,792 km</td>    </tr>    <tr>      <th>Polar Diameter:</th>      <td>6,752 km</td>    </tr>    <tr>      <th>Mass:</th>      <td>6.42 x 10^23 kg (10.7% Earth)</td>    </tr>    <tr>      <th>Moons:</th>      <td>2 (Phobos &amp; Deimos)</td>    </tr>    <tr>      <th>Orbit Distance:</th>      <td>227,943,824 km (1.52 AU)</td>    </tr>    <tr>      <th>Orbit Period:</th>      <td>687 days (1.9 years)</td>    </tr>    <tr>      <th>Surface Temperature:</th>      <td>-153 to 20 Â°C</td>    </tr>    <tr>      <th>First Record:</th>      <td>2nd millennium BC</td>    </tr>    <tr>      <th>Recorded By:</th>      <td>Egyptian astronomers</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape facts table\n",
    "facts_url = \"https://space-facts.com/mars/\"\n",
    "facts_table = pd.read_html(facts_url)\n",
    "facts_df = facts_table[0]\n",
    "facts_df=facts_df.set_index(0)\n",
    "\n",
    "# Convert facts dataframe to HTML\n",
    "facts_html = facts_df.to_html(header = False, index_names=False).replace(\"\\n\", \"\")\n",
    "facts_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splinter broswer instance\n",
    "executable_path = {'executable_path':'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path)\n",
    "# Open browser & visit website\n",
    "usgs_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(usgs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cerberus Hemisphere',\n",
       " 'Schiaparelli Hemisphere',\n",
       " 'Syrtis Major Hemisphere',\n",
       " 'Valles Marineris Hemisphere']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make list of hemispheres\n",
    "link_objects = browser.find_by_css('h3')\n",
    "hemisphere_list = []\n",
    "{hemisphere_list.append(link.value.replace(\" Enhanced\",\"\")) for link in link_objects}\n",
    "hemisphere_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg',\n",
       " 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg',\n",
       " 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg',\n",
       " 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make list of urls\n",
    "url_list = []\n",
    "for hemisphere in hemisphere_list:\n",
    "    browser.click_link_by_partial_text(hemisphere)\n",
    "    image_object = browser.find_by_css('img.wide-image')\n",
    "    img_url = image_object['src']\n",
    "    url_list.append(img_url)\n",
    "    browser.back()\n",
    "browser.quit()\n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dictionary\n",
    "hemisphere_image_urls = []\n",
    "for hemisphere, url in zip(hemisphere_list, url_list):\n",
    "    hemisphere_dict = {\"title\": hemisphere, \"url\": url}\n",
    "    hemisphere_image_urls.append(hemisphere_dict)\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    scrape_dict = {}\n",
    "    \n",
    "    # Update dictionary with scrape time\n",
    "    scrape_dict[\"scrape_time\"] = str(datetime.datetime.now())\n",
    "\n",
    "    # Get most current news story from NASA's mars site\n",
    "    nasa_news_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "    nasa_html = requests.get(nasa_news_url).text\n",
    "    nasa_soup = bs(nasa_html, 'lxml')\n",
    "    # Get first title\n",
    "    title_results = nasa_soup.find_all('div', class_=\"content_title\")\n",
    "    title_list = []\n",
    "    for result in title_results:\n",
    "        try:\n",
    "            title = result.find('a').text.strip()\n",
    "            if title:\n",
    "                title_list.append(title)\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    news_title = title_list[0]\n",
    "    # Get first paragraph\n",
    "    p_results = nasa_soup.find_all('div', class_=\"rollover_description_inner\")\n",
    "    p_list = []\n",
    "    for p in p_results:\n",
    "        try:\n",
    "            par = p.text.strip()\n",
    "            if par:\n",
    "                p_list.append(par)\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    news_p = p_list[0]\n",
    "    # Update dictionary\n",
    "    scrape_dict[\"mars_news_title\"] = news_title\n",
    "    scrape_dict[\"mars_news_p\"] = news_p\n",
    "\n",
    "    # Create splinter browser instance\n",
    "    executable_path = {'executable_path':'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path)\n",
    "    # Scrape NASA images page for featured image\n",
    "    nasa_images_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(nasa_images_url)\n",
    "    browser.find_by_css('.button').first.click()\n",
    "    time.sleep(3)\n",
    "    browser.find_by_css('.button').last.click()\n",
    "    partial_link = browser.find_by_css('.download_tiff').last.value.split(\" \")[2]\n",
    "    browser.click_link_by_partial_href(partial_link)\n",
    "    featured_image_url = browser.url\n",
    "    # Update dictionary\n",
    "    scrape_dict[\"featured_image\"] = featured_image_url\n",
    "\n",
    "    # Use Splinter to scrape USGS for hemisphere images and urls\n",
    "    usgs_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(usgs_url)\n",
    "    link_objects = browser.find_by_css('h3')\n",
    "    hemisphere_list = []\n",
    "    {hemisphere_list.append(link.value.replace(\" Enhanced\",\"\")) for link in link_objects}\n",
    "    url_list = []\n",
    "    for hemisphere in hemisphere_list:\n",
    "        browser.click_link_by_partial_text(hemisphere)\n",
    "        image_object = browser.find_by_css('img.wide-image')\n",
    "        img_url = image_object['src']\n",
    "        url_list.append(img_url)\n",
    "        browser.back()\n",
    "    browser.quit()\n",
    "    hemisphere_image_urls = []\n",
    "    for hemisphere, url in zip(hemisphere_list, url_list):\n",
    "        hemisphere_dict = {\"title\": hemisphere, \"url\": url}\n",
    "        hemisphere_image_urls.append(hemisphere_dict)\n",
    "    hemisphere_image_urls\n",
    "    # Update dictionary\n",
    "    scrape_dict[\"hemisphere_images\"] = hemisphere_image_urls\n",
    "    \n",
    "    # Scrape weather conditions from Mars Weather Twitter\n",
    "    twitter_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    twitter_html = requests.get(twitter_url).text\n",
    "    twitter_soup = bs(twitter_html, 'lxml')\n",
    "    tweets = twitter_soup.find_all('div', class_ = \"content\")\n",
    "    weather_only_tweets = []\n",
    "    for tweet in tweets:\n",
    "        username = tweet.find('span', class_ = \"username u-dir u-textTruncate\")\n",
    "        if username.text == \"@MarsWxReport\":\n",
    "            tweet_content = tweet.find('p', class_ = \"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text.strip()\n",
    "            report_test = tweet_content.split(\" \")\n",
    "            if report_test[0] == \"Sol\":\n",
    "                weather_only_tweets.append(tweet_content)\n",
    "    mars_weather = weather_only_tweets[0]\n",
    "    # Update dictionary\n",
    "    scrape_dict[\"mars_weather\"] = mars_weather\n",
    "    \n",
    "    # Scrape facts table\n",
    "    facts_url = \"https://space-facts.com/mars/\"\n",
    "    facts_table = pd.read_html(facts_url)\n",
    "    facts_df = facts_table[0]\n",
    "    facts_html = facts_df.to_html(header = False, index = False).replace(\"\\n\", \"\")\n",
    "    # Update dictionary\n",
    "    scrape_dict[\"mars_facts\"] = facts_html\n",
    "    \n",
    "    return scrape_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
